{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T13:27:56.715174Z","iopub.status.busy":"2023-07-02T13:27:56.714779Z","iopub.status.idle":"2023-07-02T13:27:56.723144Z","shell.execute_reply":"2023-07-02T13:27:56.722151Z","shell.execute_reply.started":"2023-07-02T13:27:56.715142Z"},"trusted":true},"outputs":[],"source":["from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n","from keras.models import Sequential\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns; sns.set()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T13:30:46.700148Z","iopub.status.busy":"2023-07-02T13:30:46.698908Z","iopub.status.idle":"2023-07-02T13:30:46.723831Z","shell.execute_reply":"2023-07-02T13:30:46.722784Z","shell.execute_reply.started":"2023-07-02T13:30:46.700100Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"dataset/data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T13:31:38.387956Z","iopub.status.busy":"2023-07-02T13:31:38.386862Z","iopub.status.idle":"2023-07-02T13:31:38.395361Z","shell.execute_reply":"2023-07-02T13:31:38.394522Z","shell.execute_reply.started":"2023-07-02T13:31:38.387917Z"},"trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T13:34:04.979760Z","iopub.status.busy":"2023-07-02T13:34:04.979329Z","iopub.status.idle":"2023-07-02T13:34:05.069120Z","shell.execute_reply":"2023-07-02T13:34:05.068068Z","shell.execute_reply.started":"2023-07-02T13:34:04.979729Z"},"trusted":true},"outputs":[],"source":["texts = []\n","labels = []\n","for i, label in enumerate(data['Category']):\n","    texts.append(data['Message'][i])\n","    if label == 'ham':\n","        labels.append(0)\n","    else:\n","        labels.append(1)\n","\n","texts = np.asarray(texts)\n","labels = np.asarray(labels)\n","\n","\n","print(\"number of texts :\" , len(texts))\n","print(\"number of labels: \", len(labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n","from keras.models import Sequential\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","\n","# number of words used as features\n","max_features = 10000\n","# cut off the words after seeing 500 words in each document(email)\n","maxlen = 500\n","\n","\n","# we will use 80% of data as training, 20% as validation data\n","training_samples = int(5572 * .8)\n","validation_samples = int(5572 - training_samples)\n","# sanity check\n","print(len(texts) == (training_samples + validation_samples))\n","print(\"The number of training {0}, validation {1} \".format(training_samples, validation_samples))\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print(\"Found {0} unique words: \".format(len(word_index)))\n","\n","data = pad_sequences(sequences, maxlen=maxlen)\n","\n","print(\"data shape: \", data.shape)\n","\n","np.random.seed(42)\n","# shuffle data\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","\n","texts_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","texts_test = data[training_samples:]\n","y_test = labels[training_samples:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(SimpleRNN(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history_rnn = model.fit(texts_train, y_train, epochs=10, batch_size=60, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acc = history_rnn.history['acc']\n","val_acc = history_rnn.history['val_acc']\n","loss = history_rnn.history['loss']\n","val_loss = history_rnn.history['val_loss']\n","epochs = range(len(acc))\n","plt.plot(epochs, acc, '-', color='orange', label='training acc')\n","plt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(epochs, loss, '-', color='orange', label='training acc')\n","plt.plot(epochs, val_loss,  '-', color='blue', label='validation acc')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","pred = predictions = (model.predict(texts_test) > 0.5).astype(\"int32\")\n","\n","acc = model.evaluate(texts_test, y_test)\n","proba_rnn = (model.predict(texts_test) > 0.5).astype(\"int32\")\n","from sklearn.metrics import confusion_matrix\n","print(\"Test loss is {0:.2f} accuracy is {1:.2f}  \".format(acc[0],acc[1]))\n","print(confusion_matrix(pred, y_test))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","email = [\" To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\"]\n","email = pad_sequences(tokenizer.texts_to_sequences(email), maxlen=maxlen)\n","\n","\n","print(model.predict(email))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
